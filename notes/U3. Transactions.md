# U3. TRANSACTIONS



# L11. Distributed Transactions

- 2-phase locking (2PL)
- 2-phase commit (2PC)

### Problem: cross-machine atomic ops

- goal: atomicity w.r.t. failures + concurrency

### Transactions

- ACID: Atomic, Consistent, Isolated, Durable
- out focus: fistributed transactions

### Isolation: Serializability

- Problem cases
  - t1 <- get(x); transfer(x, y); t2 <- get(y)
  - put(x); get(x); get(y); put(y)

### Concurrency control

1. pessimistic (locks) (today)
2. optimistic (no locks, abort if not serializable)

### Two-phase locking (lock per record)

- one way to implement serializability (single machine)
- 2PL rules
  - T only acquires lock before using
  - T holds ultil commit / abort
- Two-phase locking can produce deadlock
  - The system must detect (cycles? timeout?) and abort a transaction

### Two-phase commit: Crashes

- We want "atomic commit" (multiple machines)
- Discussion
  - Use raft to make Coordinator available
  - Raft ~ 2PC?
    - Raft: all servers do the same (RSM)
    - 2PC: all servers operate on different data
      atomic ops across servers



# L12. Frangipani

- Network file systems
- Focus (a gentle introduction)
  - cache coherence
  - distributed locking
  - distributed crash recovery

### Traditional Network FS vs Frangipani

- simple client and complex FS
- Frangipani: complex client with FS

### Use case

- Researchers: compiling, editing
- Potentially share files/dr
  - user-to-user sharing
  - same user logs inot more than 1 workstations

### Design choices

- Caching (write-back)
- Strong consistency
- Performance

### Cache coherence / consistency

- lock server
  - busy, idle

### Atomicity

- same lock server

### Crsh recovery

- write-ahead logging



## Paper

### Frangipani: A Scalable Distributed File System

1. Introduction
2. System Structure
   1. Components
   2. Security and the Client/Server Configuration
   3. Discussion
3. Disk Layout
4. Logging and Recovery
5. Synchronization and Cache Coherence
6. The Lock Service
7. Adding and Removing Servers
8. Backup
9. Performance
   1. Experimental Setup
   2. Single Machine Performance
   3. Scaling
   4. Effects of Lock Contention
10. Related Work
11. Conclusions

### *Thinking*

- 介绍了Frangipani，一个分布式文件系统，该系统首次提出了几个重要的概念
  - Cache Coherence：保证不同用户在（使用本地缓存）读写同一文件的strong consistency
    - 前设条件是大部分人都主要在自己的文件上工作，因此文件操作在本地缓存中进行
  - Distributed Locking：通过lock servers和busy、idle、write-ahead logging等机制，提供了strong consistency的同时保证了performance
    - 类似unix系统中硬盘的写操作
  - Distributed Crash Recovery：一个server崩溃后可由系统中任意server进行恢复（类似unix系统中的硬盘恢复）



# L14. Spanner

- Wide-area transactions
  - R/W transaction: 2PC + 2PL + Paxos groups
  - R/O transaction: snapshot isolation + syncronized clocks
- Widely-used

### Organization

- different data centers
- multiple shards --> parallelism
- Paxos group per shard
  - --> data center: fault tolerance, slowness
- Replica close to clients

### Challenges

- Read of local replica yields latest Write
- Transactions across Shards
- Transactions must be serializable

### Read/Write transactions



## Read-only transactions

- 10 x Faster than R/W
  - Read from local shards
  - No lock
  - No 2PC

### Correctness

- Serializable
- External consistency ~ Linearizability (transaction version)

### Snapshot Isolation

- Timestamp

### Stale Replica

- Solution: "safte time"
  - Paxos sends writes in timestamp order
  - Before Rx@15, wait for Wx>@15
    (Also wait for transactions that have prepared but not committed)

### Clocks must be perfect

- Matters only for R/O transactions
- Difficulty: clock is naturally drifty
  - --> atomic clocks --> synchronize clocks (gps) --> $\epsilon$ few microsec to few millisec
- Solution: time stamps are intervals
  - [earliest, latest]
- Rule change
  - Start rule: `timestamp = now.latest`
  - Commit wait: delay the commit until `timestamp < now.earlist`

### Discussion

- R/W transaction: 2PC + 2PL
- R/O transaction: 
  - snapshot isolation 
  - --> external consistency
  - --> time stamp order
  - --> time intervals



## Paper

### Spanner: Google’s Globally-Distributed Database

1. Introduction
2. Implementation
   1. Spanserver Software Stack
   2. Directories and Placement
   3. Data Model
3. True Time
4. Concurrency Control
   1. Timestamp Mangagement
      1. Paxos Leader Leases
      2. Assigning Timestamps to RW Transactions
      3. Serving Reads at a Timestamp
      4. Assigning Timestamps to RO Transactions
   2. Details
      1. Read-Write Transactions
      2. Read-Only Transactions
      3. Schema-Change Transactions
      4. Refinements
5. Evaluation
   1. Microbenchmarks
   2. Availability
   3. TrueTime
   4. F1
6. Related Work
7. Future Work
8. Conclusions



# L15. FaRM, Optimistic Concurrency Control

### Overview

- High performance transactions
  - 140M/s (90 machines)
- One data center
- Strict Serializability
- Sharding
- NV DRAM
- Kernel-by-pass + RDMA
- Optimistic concurrency control
- Research prototype

### Setup

- configuration manager: zoo keeper
  - mapping: region# --> [primary, backup]
- sharded w/ primary/backup replication

### API

- object -> oid [region#, offset]
  - 64-bit number: lock bit + 63-bit v#

```pseudocode
txbegin
	o = Read(oid)
	o.f += 1
	Write(oid, o)
txcommit
```

### Kernel-bypass

- dpdk
- RDMA (remote direct memory access) (w/ NIC support)
  - read RDMA (one-sided, no server involved)
  - write RDMA

### Challenge: transactions using RDMA

- Protocols for TXN, 2PC, ...
  - server-side participation
  - --> run code on the server
- Solution: use optimistic concurrency control
  - read objs without locking (version#)
  - validation step for conflict
    - version# different --> abort
    - same --> commit

### Optimistic Concurrency Control

- ​	<img src="image.assets/Screen Shot 2022-08-02 at 14.57.26.png" alt="Screen Shot 2022-08-02 at 14.57.26" style="zoom:33%;" />

### Summary

- Fast
  - Assumes few conflict
  - Data must fit in memory
  - Replication is only within the data center
  - Requires fancy hardware: UPS, RDMA NIC

## Paper

### No compromises: distributed transactions with consistency, availability, and performance

<img src="image.assets/Screen Shot 2022-08-03 at 10.22.53.png" alt="Screen Shot 2022-08-03 at 10.22.53" style="zoom: 50%;" />

1. Introduction
2. Hardware trends
   1. Non-volatile DRAM
   2. RDMA networking (remote direct memory access)
3. Programming model and architecture
4. Distributed transactions and replication
5. Failure recovery
   1. Failure detection
   2. Reconfiguration
   3. Transaction state recovery
   4. Recovering data
   5. Recovering allocator state
6. Evaluation
   1. Setup
   2. Benchmarks
   3. Normal-case performance
   4. Failures
   5. Lease times
7. Related work
8. Conclusion

### *Summary*

- Performance
  - Hardware trends
    - Non-volatile DRAM (with UPS)
    - RDMA NIC
  - Optimistic concurrency control
    - RDMA compatible protocol (vs. 2PC) for lock-free read
- Consistency
  - Strict Serializability

### *Thinking*

- 介绍了微软的一个研究项目FaRM，同时保证consistency，availability和performance，其中性能是关键（140M Transactions/sec w/ 90 machines）
  - 所有数据存储在DRAM中，利用UPS保证NV
  - 使用RDMA提高不同机器间的读写速度，使用Optimistic concurrency control充分利用RDMA的性能（lock-free read）
    - optimistic
      - optimistic1：从servers获取数据并在本地进行数据处理
      - optimistic2：之后依次获取write操作的lock，对read操作进行validation，任意失败则abort
    - read只需两次one-sided RDMA，不涉及任何server os的操作（interrupt）
      - 需要两次是因为如果一个read-only transaction中包含多个read，则需要validation保证中间不被其他transaction改变数据
      - 如果只包含一个read实际上只需要一个one-sided RDMA（FaRM未必如此实现，也可能有其他细节问题）





















