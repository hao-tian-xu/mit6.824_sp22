# U4. OTHER TOPICS



# L16. Spark

### Distributed Computation

- Successor to Hadoop (open-source MapReduce)
- Wider range of applications (than MapReduce)
- In-memory computations
- Widely used (databricks, apache spark)

### Programming model: RDD

### Summary

- RDDs by functional transformations
- Lineage graph: reuse, clever optimizations
- More expressiveness than MapReduce
- In memory --> performance

## Paper

### Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing

<img src="image.assets/Screen Shot 2022-08-03 at 10.21.50.png" alt="Screen Shot 2022-08-03 at 10.21.50" style="zoom: 50%;" />

1. Introduction
2. Resilient Distributed Datasets (RDDs)
   1. RDD Abstraction
   2. Spark Programming Interface
      1. Example: Console Log Mining
   3. Advantages of the RDD Model
   4. Applications Not Suitable for RDDs
3. Spark Programming Interface
   1. RDD Operations in Spark
   2. Example Applications
      1. Logistic Regression
      2. PageRank
4. Representing RDDs
5. Implementation
   1. Job Scheduling
   2. Interpreter Integration
   3. Memory Management
   4. Support for Checkpointing
6. Evaluation
   1. Interative Machine Learning Applications
   2. PageRank
   3. Fault Recovery
   4. Behavior with Insufficient Memory
   5. User Applications Built with Spark
   6. Interactive Data Mining
7. Discussion
   1. Expressing Existing Programming Models
   2. Leveraging RDDs for Debugging
8. Related Work
9. Conclusion

### *Summary*

- Topic: Distributed Computation
  - Basically generalized in-memory MapReduce 

- Lineage graph for computation and fault tolerance
  - Optimizations
    - Checkpointing for quick recovery
    - Custom partition functions for less wide dependencies

### *Thinking*

- MapReduce的继任
  - 性能：中间结果保存在内存中，保证了最少量的硬盘读写
    - 通过记录Lineage graph在错误时重新计算
  - 表达性：提供除Map和Reduce之外更多的transformation，通过Lineage graph记录和表达计算关系
- 对于MapReduce非常直接易懂的优化，但非常powerful



# L16. Scaling Memcache at Facebook

### Exprience paper

- impressive performance
- tension between performance and consistency
- cautionary tales

### Website evolution

1. single machine w/ web server + application + DB\
2. many web FEs, one shared DB
3. many web FEs, data sharded over cluster of DBs
4. many web FEs, many caches for reads, many DBs for writes
   1. DB + cache consistency
   2. avoid DB overload

### Consistency: eventual

- Writes ordering (database)
- Reads are behind: ok
  - except: clients read their own writes
- similar to ZooKeeper

### Invalidation of caches

- delete cache when put
- look-aside cache

### Reads / Writes 

- paper figure 2

### Performance approaches

- partition / shard data
  - ++ capacity
  - ++ parallelism
- replicate data
  - ++ hot key
  - -- capacity
- replicate within a single data center
  - ++ hot key
  - ++ reduce #connection
    - --> avoid incast congestion
    - --> reduce network pressure
  - -- capacity
    - --> custom regional pool

### Performance challenges / Protecting database

- New cluster
  - get from existing cluster
  - set in new cluster
- Thundering herd
  - lease
- Memcached server failure
  - gutter pool

### Race

- stale set
  - lease
- cold cluster
  - 2s hold off (during warm-up)
- regions (primary / backup)
  - "remote" tag

### Summary

- Caching is vital
  - Partitioning / Sharding
  - Replication
- Consistency between DB and memcaches (tricky)

## Paper

### Scaling Memcache at Facebook

<img src="image.assets/Screen Shot 2022-08-04 at 15.59.54.png" alt="Screen Shot 2022-08-04 at 15.59.54" style="zoom:50%;" />

1. Introduction
2. Overview
3. In a Cluster: Latency and Load
   1. Reducing Latency
      - Parallel requests and batching, Client-server communication, Incast congestion
   2. Reducing Load
      1. Leases
      2. Memcache Pools
      3. Replication Within Pools
   3. Handling Failures
4. In a Region: Replication
   1. Regional Invalidations
   2. Regional Pools
   3. Cold Cluster Warmup
5. Across Regions: Consistency
6. Single Server Improvements
   1. Performance Optimizations
   2. Adaptive Slab Allocator
   3. The Transient Item Cache
   4. Software Upgrades
7. Memcache Workload
   1. Measurements at the Web Server
   2. Pool Statistics
   3. Invalidation Latency
8. Related Work
9. Conclusion

### *Summary*

- Topic: cache consistency
- Consistency: Eventual
  - Writes ordering
  - Clients read their own write
- Performance: optimization for read throughput
  - Memcache: as a demand-filled look-aside cache
    - Overall: sharding + replication
    - Hot key: replication + custom pool (for capacity)

### *Thinking*

- 介绍了Facebook使用Memcache来提高读操作的速度和吞吐量
  - 弱consistency（eventual）提高性能
  - 写操作删除cache保证eventual consistency
- 不同scale的服务使用memcache提高性能的经验，以及解决随之带来的race的方法



# L17. Causal Consistency, COPS

### Setting

- multiple datacenters
- each datacenter has a complete copy of all data 
- local Reads + local Writes
- consistency follow performance

### Straw man I: eventual consistency

- Reads and Writes locally
- send writes to other datacenters asynchronously (maybe not in order)
- Examples: service
  - Dynamo
  - Cassandra
- Example: app
  - c1: `put(photo)`, `put(list)`
  - c2: `get(list)`, `get(photo)`
    - problem: `list` maybe updated before `photo`

### Lamport clocks (clock inconsistency)

- Tmax = highest version# seen
- version#: T = max(Tmax + 1, real time)

### Straw man II: eventual consistency + barriers

- `sync(k, v#)`
  - does not return until: every datacenter has at least v# for k
- `put(k, v)` --> `v#`
- example: app
  - c1: `put(photo)`, `sync(photo, v#)`, `put(list)`
  - c2: `get(list)`, `get(photo)`

### Log: no `sync()`

- single write log per datacenter
  - problem: writes not sent to other servers immediately

## COPS

- client context
  - problem: cascading wait

### Causal Consistency

- paper figure 2

- Transaction

### Summary

- a popular research idea
  - with good reason: promises both performance and useful consistency
- rarely used in deployed storage systems
- what is actually used?
  - no geographic replication at all, just local
  - primary-site (PNUTS, Facebook/Memcache)
  - eventual consistency (Dynamo, Cassandra)
  - strongly consistent (Spanner)

## Paper

### Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS

<img src="image.assets/Screen Shot 2022-08-04 at 18.40.29.png" alt="Screen Shot 2022-08-04 at 18.40.29" style="zoom:33%;" />

1. INTRODUCTION
2. ALPS SYSTEMS AND TRADE-OFFS
3. CAUSAL+ CONSISTENCY
   1. Definition
   2. Causal+ vs. Other Consistency Models
   3. Causal+ in COPS
   4. Scalable Causality
4. SYSTEM DESIGN OF COPS
   1. Overview of COPS
   2. The COPS Key-Value Store
   3. Client Library and Interface
   4. Writing Values in COPS and COPS-GT
   5. Reading Values in COPS
   6. Get Transactions in COPS-GT
5. GARBAGE, FAULTS, AND CONFLICTS
   1. Garbage Collection Subsystem
   2. Fault Tolerance
   3. Conflict Detection
6. EVALUATION
   1. Implementation and Experimental Setup
   2. Microbenchmarks
   3. Dynamic Workloads
   4. Scalability
7. RELATED WORK
8. CONCLUSION

### *Summary*

- Multiple Datacenters
- Consistency: Causal, Causal+
  - Client context
- Performance
  - Local Reads / Writes

### *Thinking*

- 介绍了Causal Consistency
  - 利用Client Context，在Eventual Consistency的基础上增加了一些有用的spec
    - 如不同的datacenters之间：一个Client顺序修改了两个kv pairs，另一个Client不会读到了后修改的，之后却没有读到先修改的

















