# U4. OTHER TOPICS



# L16. Spark

### Topic: Distributed Computation

- Successor to Hadoop (open-source MapReduce)
- Wider range of applications (than MapReduce)
- In-memory computations
- Widely used (databricks, apache spark)

### Programming model: RDD

### Summary

- RDDs by functional transformations
- Lineage graph: reuse, clever optimizations
- More expressiveness than MapReduce
- In memory --> performance

## Paper

### Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing

<img src="image.assets/Screen Shot 2022-08-03 at 10.21.50.png" alt="Screen Shot 2022-08-03 at 10.21.50" style="zoom: 50%;" />

1. Introduction
2. Resilient Distributed Datasets (RDDs)
   1. RDD Abstraction
   2. Spark Programming Interface
      1. Example: Console Log Mining
   3. Advantages of the RDD Model
   4. Applications Not Suitable for RDDs
3. Spark Programming Interface
   1. RDD Operations in Spark
   2. Example Applications
      1. Logistic Regression
      2. PageRank
4. Representing RDDs
5. Implementation
   1. Job Scheduling
   2. Interpreter Integration
   3. Memory Management
   4. Support for Checkpointing
6. Evaluation
   1. Interative Machine Learning Applications
   2. PageRank
   3. Fault Recovery
   4. Behavior with Insufficient Memory
   5. User Applications Built with Spark
   6. Interactive Data Mining
7. Discussion
   1. Expressing Existing Programming Models
   2. Leveraging RDDs for Debugging
8. Related Work
9. Conclusion

### *Summary*

- Topic: Distributed Computation
  - Basically generalized in-memory MapReduce 

- Lineage graph for computation and fault tolerance
  - Optimizations
    - Checkpointing for quick recovery
    - Custom partition functions for less wide dependencies

### *Thinking*

- MapReduce的继任
  - 性能：中间结果保存在内存中，保证了最少量的硬盘读写
    - 通过记录Lineage graph在错误时重新计算
  - 表达性：提供除Map和Reduce之外更多的transformation，通过Lineage graph记录和表达计算关系
- 对于MapReduce非常直接易懂的优化，但非常powerful



# L16. Scaling Memcache at Facebook

### Topic: Cache Consistency

### Exprience paper

- impressive performance
- tension between performance and consistency
- cautionary tales

### Website evolution

1. single machine w/ web server + application + DB\
2. many web FEs, one shared DB
3. many web FEs, data sharded over cluster of DBs
4. many web FEs, many caches for reads, many DBs for writes
   1. DB + cache consistency
   2. avoid DB overload

### Consistency: eventual

- Writes ordering (database)
- Reads are behind: ok
  - except: clients read their own writes
- similar to ZooKeeper

### Invalidation of caches

- delete cache when put
- look-aside cache

### Reads / Writes 

- paper figure 2

### Performance approaches

- partition / shard data
  - ++ capacity
  - ++ parallelism
- replicate data
  - ++ hot key
  - -- capacity
- replicate within a single data center
  - ++ hot key
  - ++ reduce #connection
    - --> avoid incast congestion
    - --> reduce network pressure
  - -- capacity
    - --> custom regional pool

### Performance challenges / Protecting database

- New cluster
  - get from existing cluster
  - set in new cluster
- Thundering herd
  - lease
- Memcached server failure
  - gutter pool

### Race

- stale set
  - lease
- cold cluster
  - 2s hold off (during warm-up)
- regions (primary / backup)
  - "remote" tag

### Summary

- Caching is vital
  - Partitioning / Sharding
  - Replication
- Consistency between DB and memcaches (tricky)

## Paper

### Scaling Memcache at Facebook

<img src="image.assets/Screen Shot 2022-08-04 at 15.59.54.png" alt="Screen Shot 2022-08-04 at 15.59.54" style="zoom:50%;" />

1. Introduction
2. Overview
3. In a Cluster: Latency and Load
   1. Reducing Latency
      - Parallel requests and batching, Client-server communication, Incast congestion
   2. Reducing Load
      1. Leases
      2. Memcache Pools
      3. Replication Within Pools
   3. Handling Failures
4. In a Region: Replication
   1. Regional Invalidations
   2. Regional Pools
   3. Cold Cluster Warmup
5. Across Regions: Consistency
6. Single Server Improvements
   1. Performance Optimizations
   2. Adaptive Slab Allocator
   3. The Transient Item Cache
   4. Software Upgrades
7. Memcache Workload
   1. Measurements at the Web Server
   2. Pool Statistics
   3. Invalidation Latency
8. Related Work
9. Conclusion

### *Summary*

- Topic: cache consistency
- Consistency: Eventual
  - Writes ordering
  - Clients read their own write
- Performance: optimization for read throughput
  - Memcache: as a demand-filled look-aside cache
    - Overall: sharding + replication
    - Hot key: replication + custom pool (for capacity)

### *Thinking*

- 介绍了Facebook使用Memcache来提高读操作的速度和吞吐量
  - 弱consistency（eventual）提高性能
  - 写操作删除cache保证eventual consistency
- 不同scale的服务使用memcache提高性能的经验，以及解决随之带来的race的方法



# L17. Causal Consistency, COPS

### Topic: Causal Consistency

### Setting

- multiple datacenters
- each datacenter has a complete copy of all data 
- local Reads + local Writes
- consistency follow performance

### Straw man I: eventual consistency

- Reads and Writes locally
- send writes to other datacenters asynchronously (maybe not in order)
- Examples: service
  - Dynamo
  - Cassandra
- Example: app
  - c1: `put(photo)`, `put(list)`
  - c2: `get(list)`, `get(photo)`
    - problem: `list` maybe updated before `photo`

### Lamport clocks (clock inconsistency)

- Tmax = highest version# seen
- version#: T = max(Tmax + 1, real time)

### Straw man II: eventual consistency + barriers

- `sync(k, v#)`
  - does not return until: every datacenter has at least v# for k
- `put(k, v)` --> `v#`
- example: app
  - c1: `put(photo)`, `sync(photo, v#)`, `put(list)`
  - c2: `get(list)`, `get(photo)`

### Log: no `sync()`

- single write log per datacenter
  - problem: writes not sent to other servers immediately

## COPS

- client context
  - problem: cascading wait

### Causal Consistency

- paper figure 2

- Transaction

### Summary

- a popular research idea
  - with good reason: promises both performance and useful consistency
- rarely used in deployed storage systems
- what is actually used?
  - no geographic replication at all, just local
  - primary-site (PNUTS, Facebook/Memcache)
  - eventual consistency (Dynamo, Cassandra)
  - strongly consistent (Spanner)

## Paper

### Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS

<img src="image.assets/Screen Shot 2022-08-04 at 18.40.29.png" alt="Screen Shot 2022-08-04 at 18.40.29" style="zoom:33%;" />

1. INTRODUCTION
2. ALPS SYSTEMS AND TRADE-OFFS
3. CAUSAL+ CONSISTENCY
   1. Definition
   2. Causal+ vs. Other Consistency Models
   3. Causal+ in COPS
   4. Scalable Causality
4. SYSTEM DESIGN OF COPS
   1. Overview of COPS
   2. The COPS Key-Value Store
   3. Client Library and Interface
   4. Writing Values in COPS and COPS-GT
   5. Reading Values in COPS
   6. Get Transactions in COPS-GT
5. GARBAGE, FAULTS, AND CONFLICTS
   1. Garbage Collection Subsystem
   2. Fault Tolerance
   3. Conflict Detection
6. EVALUATION
   1. Implementation and Experimental Setup
   2. Microbenchmarks
   3. Dynamic Workloads
   4. Scalability
7. RELATED WORK
8. CONCLUSION

### *Summary*

- Multiple Datacenters
- Consistency: Causal, Causal+
  - Client context
- Performance
  - Local Reads / Writes

### *Thinking*

- 介绍了Causal Consistency
  - 利用Client Context，在Eventual Consistency的基础上增加了一些有用的spec
    - 如不同的datacenters之间：一个Client顺序修改了两个kv pairs，另一个Client不会读到了后修改的，之后却没有读到先修改的



# L18. Secure Untrusted Data Repository (SUNDR) (2004)

### Topic: Decentralized systyems

- Byzantine participants
- Security
- Powerful: signed log
  - sundr, git, bitcoin, keybase (zoom)

### Setting: network file system

- Like Franginpani, but clients untrusted
- Possible real attacks
  - Bugs in server software.
  - Administrators w/ weak passwords.
  - Physical break-ins at data center.
  - Malicious or bribed server operators.

### Focus: integrity

### Naive Solution

- Too simple design: file signature
  - cannot capture relationships of files

### Big idea: signed log of operations

- log entry covers all preceding entries
  - --> server cannot drop A and keep B's modification

- Client:
  - check signatures
  - check its own last entry
  - construct file system
  - add entry log and sign
  - upload log to file server

- why fetch in the log

### Fork consistency

### Detecting forks

- out-of-band communication
- time stamp box

### Snapshot per user

- per user: i-handle
- across users: version vectors
  - vs_A: [A's i-handle, {A: 1, B: 0, C:0}]
  - vs_B: [B's i-handle, {A: 1, B: 1, C:0}]
  - C: auth.py + bank.py

### Summary

- Byzantine participants in decentralized systems
- Signed log
- Fork consistency



## Paper

### Secure Untrusted Data Repository (SUNDR)

1. Introduction
2. Setting
3. The SUNDR protocol
   1. A straw-man file system
   2. Implications of fork consistency
   3. Serialized SUNDRA
      1. Data structures
      2. Protocol
   4. Concurrent SUNDR
      1. Update certificates
      2. Update confilicts
      3. Example
4. Discussion
5. File system implementation
   1. File system client
   2. Signature optimization
   3. Consistency server
6. Block store implementation
   1. interface
   2. Index
   3. Data manamgement
7. Performance
   1. Experimental setup
   2. Microbenchmarks
      1. bstor
      2. Cryptographic overhead
   3. End-to-end evaluation
      1. LFS small file benchmark
      2. Group contention
      3. Real workloads
      4. CVS on SUNDR
8. Related work
9. Conclusions

### *Summary*

- Decentralized system with untrusted participants
- Attack detection
  - Signed log of operations
  - --> improvement: Snapshot (i-handle + version vector)
- Consistency: Fork consistency

### *Thinking*

- 介绍了一个去中心化系统SUNDR，提出了一些去中心化系统的重要概念和技术
  - Fork consistency：去中心化系统中面对malicious server的一致性保证——server在fork之后不能在client不知情的情况下merge
  - Signed log：最重要的保证fork consistency的技术
  - Signature：提供去中心化系统中的权限和信息验证



# L19. Bitcoin

### Topic: Consensus with Byzantine participants

- Signed log of transactions, Handle forks
- Unusual: pseudo-anonymous

### Challenges

- Outright forgery (solution: signning)
- Double spending (solution: public log -> consensus)
- Theft (steal private key)

### Ledger record/transaction (simplified)

- Public key v1 <- dst
  Hash(prev)
  Sign w. prev key v2 <- src
  Amount, many in/out

### Example

- T6: pub(x), ...
- T7: pub(y), hash(T6), sig(x)
- T8: pub(z), hash(T7), sig(y)
  - z verifies: ship goods to y

### Problem: Double spending

- y creates two transaction records

### Solution: public log (ordered all transactions)

- look in log: has T7 already been spent? --> if so, reject

### Problem: agreement on the log

- Design 1
  - trusted server --> want decentralization
- Design 2
  - use SUNDR --> different views of the world (fork)

### Decentralized agreement

- Raft agreement --> Open: no majority

### Bitcoin: Proof of Work

- winner in PoW decides on  next log entry
- hard to impersonate winner
- --> enery waste
  - --> Proof of Stake

### Block chain

- Block
  - Hash(B-1)
    Transactions
    Nonce
    Timestamp

### New block: PoW (miners)

- Hash(B) has N leading zeros
  - On average ~ 1 CPU month

### Blocks and transactions

### Forks

- Possble problems:
  - find nonce at the same time
  - slow network
- Solution: Peer switches to longest fork

### Double spending

- y sends y->z, y->q to peers
  - --> miner reject
- y sends y->z to some peers, sends y-> q to other peers
  - unlikely for the attackers to compute a longer chain than good people

### Miner incentive

- first transaction in block is reward miner

### Arms race

- miner pools
- special hardwares
- high-speed links
- ...

### Practical issues

- 10 mins: 10 x time to flood net
- block size: determines #transactions/sec
- changes require consensus
  - some are easy (#leading 0)
  - some changes in soft fork
  - some changes in hard fork (block size)
    - --> more than one bitcoin chains

### Summary

- Distributed consensus w/ Byzantine participants
- Public ledger
- Proof of Work

## Paper

### Bitcoin: A Peer-to-Peer Electronic Cash System

1. Introduction
2. Transactions
3. Timestamp Server
4. Proof-of-Work
5. Network
6. Incentive
7. Reclaiming Disk Space
8. Simplified Payment Verification
9. Combining and Splitting Value
10. Pivacy
11. Calculations
12. Conclusion

### *Summary*

- Decentralized consensus
- Proof-of-Work
  - <-- Incentives

### *Thinking*

- 介绍了bitcoin，一种去中心化交易共识的数字加密货币
  - 核心算法是Proof-of-Work：拥有高算力的区块链获得延长速度，并因此获得认可
  - simplicity --> robustness





































