# U2. FAULT TOLERANCE STORAGE SYSTEMS



# L5. Fault Tolerance: Raft (1)

### Pattern: single point of failure

- MR replicates computation but relies on a single master to organize

* GFS replicates data but relies on the master to pick primaries

- VMware FT replicates service but relies on test-and-set to pick primary
- -> all rely on a single entity to make critical decisions to avoid split brain

### Idea: test-and-set server replication

- problem: split brain

### Network partition: majority rule

- overlap in majorities

### Protocols using Quorum

- around 1990
  - Paxos
  - View-stamped replication
- Raft (2014)

## Raft Overview

### Election

- split vote
  - random timeout
- votes are written in disk (one vote per term -> one leader per term)

### Logs may diverge

### Lab

- all things in figure 2 should be implemented



# L7. Fault Tolerance: Raft (2)

### Divergence (cont.)

### Log Catchup

subtlety in commit rule

### Service recovery

### Using Raft

### Correctness: Linearizability

- total order of operations
- match real-time
- read return results of the last write



## Paper

### In Search of an Understandable Consensus Algorithm (Extended Version)

1. **Introduction**
   - understandability: decomposition and state space reduction
2. **Replicated state machines**
   - consensus algorithm: keep the replicated log consistent
3. **What's wrong with Pxos**
4. **Designing for understandability**
5. **The Raft concensus algorithm**
   1. Raft basics
   2. Leader election
      - understandability -> randomized retry than a ranking system (subtle corner cases)
   3. Log replication
   4. Safety
      1. Election restriction
      2. Committing entries from previous terms
         - To eliminate problems like the one in Figure 8, Raft never commits log entries from previous terms by count- ing replicas.
      3. Safety argument
   5. Follower and candidate crashes
   6. Timing and availability
6. **Cluster membership changes**
7. **Log compaction**
8. **Client interaction**
9. **Implementation and evaluation**
   1. Understandability
   2. Correctness
   3. Performance
10. **Related work**
11. **Conclusion**

### *Summary*

- Fault Tolerance
  - **Majority Vote (Distributed Consencus)
  - Command Log --> RSM
  - Persistence / Snapshot --> fast recovery
- Consistency
  - Linearizability

### *Thinking-1 (to section 5)*

- 这篇论文介绍了一个共识算法Raft
  - 主要机制是Majority Vote，保证了一个commited log entry出现在未来的所有leader的log中，因此保证了所有server的state machine所应用的同一编号的log entry所包含的内容是相同的，即state machine safety
  - Raft一个关键的设计特征是可理解性，因此在关键的leader election的过程中，在一次选举失败后，采用了随机重试的方法（避免各种corner case）
    - 随机和概率（当代性）替代了确定性

### *Thinking-2 (section 7 to end)*

- 两个研究/论文的方法
  - 由于raft算法设计的目的是understandability，如何对这个相对主观的标准进行对比评价
  - 使用TLA+ specification language来做出正式的specification，并由该formal spec证明其correctness



# L9. Zookeeper Case Study

Main Purpose: configuration / coordination

### Throughput

- asynchronous writes --> single write to disk with multiple `Put()`s
- read is not linearizable --> `Get()`s from different servers

###  ZK guarantees

- Linearizable Writes
- FIFO client order
  - Writes - client-specified order (in  overall asynchronous order)
  - Reads

### Motivation

- Test-and-Set
- Configuration Info
- Master Election

### API

- znodes

  - regular
  - ephemeral
  - sequential

- RPCs

  - create(path, data, flags)
    - exclusive -- only first create indicates success
  - delete(path, version)
    - if znode.version = version, then delete
  - exists(path, watch)
    - watch=true means also send notification if path is later created/deleted
  - getData(path, watch)
  - setData(path, data, version)
    - if znode.version = version, then update
  - getChildren(path, watch)
  - sync()
    - sync then read ensures writes before sync are visible to same client's read
    - client could instead submit a write

- Example: add one to a number stored in a ZooKeeper znode ("mini-transaction")

  - ```pseudocode
    while true:
        x, v := getData("f")
        if setData(x + 1, version=v):
          break
    ```

- Herd effects

  - N^2^ complexity
  - Example: Locks without Herd Effect ("scalable lock")
    (look at pseudo-code in paper, Section 2.4, page 6)
    1. create a "sequential" file
    2. list files
    3. if no lower-numbered, lock is acquired!
      4. if exists(next-lower-numbered, watch=true)
         1. wait for event...
      5. goto 2



## Paper

### ZooKeeper: Wait-free coordination for Internet-scale systems

<img src="image.assets/Screen Shot 2022-08-03 at 10.26.56.png" alt="Screen Shot 2022-08-03 at 10.26.56" style="zoom:50%;" />

1. Introduction
2. The ZooKeeper service
   1. Service overview
   2. Client API
   3. ZooKeeper guarantees
   4. Examples of primitives
3. ZooKeeper Appliations
4. ZooKeeper Implementation
   1. Request Processor
   2. Atomic Broadcast
   3. Replicated Database
   4. Client-Server Interactions
5. Evaluation
   1. Throughput
   2. Latency of requests
   3. Performance of barriers
6. Related work
7. Conclusions

### *Summary*

- Performance
  - Wait-free reads
- Consistency
  - Linearizable Writes
  - FIFO client order
  - Client-controled consistency level (version#)

### *Thinking*

- 介绍了一个基于Zab（类似Raft）的无等待协调服务，由于对象是互联网规模的系统，因此性能是关键因素
  - 提供了两个顺序的保障
    - 一个是写操作是linearizable的（但读操作不是，因此性能和服务器数量是线性相关的）
    - 另一个是单个client的操作是按顺序的
  - 通过znode的类型以及版本号让需要strong consistency的client可以自行实现mini-transaction或lock等操作
    - 由于对象是互联网规模的系统，也提出了client避免herd effect的方法



# L10: Chain Replication

### Approaches to RSMs

1. Run all ops through Raft/Paxos (Lab3)
2. Configuration service + Primary/Backup replication (Common)

### Chain replication

- Purpose: P/B replication for approach 2
- Pros
  - Reads ops involve 1 server
  - Simple recovery plan
  - Linearizability
  - Influential

### Overview

- Servers in a chain: Head and Tail

### CR properties (wrt. Raft)

- Pros:
  - Client RPCs split between head and tail
  - Head sends update once
  - Read ops involve only tail
  - Simple crash recovery
- Cons:
  - One failure requires reconfiguration
  - Slow server in Chain Replication is very damaging

### Extension for read parallelism

- Splits object across many chains
  - CH1: S1 S2 S3
  - CH2: S2 S3 S1
  - CH3: S3 S1 S2
- --> scale + linearizability

## Paper

### Chain Replication for Supporting High Throughput and Availability

<img src="image.assets/Screen Shot 2022-08-03 at 10.28.08.png" alt="Screen Shot 2022-08-03 at 10.28.08" style="zoom:50%;" />

1. Introduction
2. A Storage Service Interface
3. Chain Replication Protocol
   1. Protocol Details
4. Primary/Backup Protocols
5. Simulation Experiments
   1. Single Chain No Failures
   2. Multiple Chains, No Failures
   3. Effects of Failures on Throughput
   4. Large Scale Replication of Critical Data
6. Related Work
7. Concluding Remarks

### *Summary*

- Consistency
  - Linearizability
- Performance
  - Chain replication

### *Thinking*

- 介绍了链式复制：一个可以同时支持高吞吐量和Linearizability的分布式存储
  - 通过链式复制write操作产生的状态改变，以及所有reply都由tail发出保证了Linearizability
  - 通过将数据分布在不同的链中，从而读操作由不同的tail完成，保证了Scalability和高吞吐量



