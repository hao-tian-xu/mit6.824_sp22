# U1. INTRODUCTIONS



# L1. Introduction

### Introduction

- Reason
  - Parallelism
  - Fault tolerance
  - Physical
  - Security / Isolated
- Challenge
  - Concurrency
  - Partial Failure
  - Performance

- Course Structure
  - Lectures: big ideas, case study
  - Papers: what the basic problems are, what the ideas are
    - how to read a paper rapidly and skip over the parts that maybe aren't that important and sort of focus on teasing out the important ideas
  - Exams
  - Labs
  - Project (optional)



### Concepts

- Infrastructure for Applications
  - Storage
  - Communication
  - Computation

- Implementation
  - RPC, threads, concurrency control
- Performance
  - Scalability: 2 x computers -> 2 x throughput
- Fault tolerance
  - Availability
  - Recoverability
    - Non-volatile storage
    - Replication
- Consistency
  - `Put(k, v)`, `Get(k) -> v`: multiple copies
- Tradeoffs
  - Fault-tolerance, consistency, and performance are enemies.
  - Many designs provide only weak consistency, to gain speed.


### MapReduce

- Job: Map tasks, Reduce tasks

## Paper

### MapReduce: Simplified Data Processing on Large Clusters

<img src="image.assets/Screen Shot 2022-06-14 at 12.56.44.png" alt="Screen Shot 2022-06-14 at 12.56.44" style="zoom: 50%;" />

1. **Introduction**

2. **Programming Model**

   1. Example

   2. Types

   3. More Examples:

      Distributed Grep, Count of URL Access Frequency, Reverse Web-Link Graph, Term-Vector per Host, Inverted Index, Distributed Sort

3. **Implementation**

   1. Execution Overview

   2. Master Data Structures

   3. Fault Tolerance

      Worker Failure, Master Failure, Semantics in the Presence of Failures

   4. Locality

   5. Task Granularity

   6. Backup Tasks

4. **Refinement**

   1. Partitioning Function
   2. Ordering Guarantees
   3. Combiner Function
   4. Input and Output Types
   5. Side-effects
   6. Skipping Bad Records
   7. Local Execution
   8. Status Information
   9. Counters

5. **Performance**

   1. Cluster Configuration
   2. Grep
   3. Sort
   4. Effect of Backup Tasks
   5. Machine Failures

6. **Experience**

   1. Large-Scale Indexing

7. **Related Work**

8. **Conclusions**

### *Summary*

- Performance
  - Backup Tasks
- Fault Tolerance

### *Thinking*

- 这篇论文介绍了Google在2003年开发的一个分布式计算框架MapReduce
  - 一个关键的意义是让不熟悉分布式系统的程序员也可以使用，也因为框架提供了简单的接口，使得Google的indexing代码变得易读和易修改
  - 介绍了分布式系统的一些基本要求如何得到满足，如Fault Tolerance，Consistency等
  - 使用Backup Tasks来解决计算的最后阶段一些计算机运行过慢的问题（straggler）



# L2. Remote Procedure Call and Threads

### Go

- Why Go
  - Good support for threads
    - Garbage collection
- Tutorial: Effective Go



### Threads

- Go routine

- Why threads

  - I/O concurrency

  - Parallelism
  - Convenience



### Thread Challenges

- Race
  - Lock
  - or avoid sharing mutable data
- Coordination
  - Go channels and/or `sync.Cond` or `sync.WaitGroup`
- Deadlock
  - Cycles via locks and/or communication



### Web Crawler

- When to use sharing and locks, versus channels?
  - Most problems can be solved in either style
    - state -- sharing and locks
    - communication -- channels
  - For the 6.824 labs, I recommend 
    - sharing+locks for state,
    - and `sync.Cond` or channels or `time.Sleep()` for waiting/notification.



## Remote Procedure Call (RPC)

- Overview
  - a key piece of distributed system machinery; all the labs use RPC
  - goal: easy-to-program client/server communication
  - hide details of network protocols
  - convert data (strings, arrays, maps, &c) to "wire format"
  - portability / interoperability
- RPC message diagram
  - Client -> request -> Server
  - Server -> response -> Client

### Software structure

```
  client app        handler fns
   stub fns         dispatcher
   RPC lib           RPC lib
     net  ------------ net
```

### Go example: `kv.go` on schedule page

- A toy key/value storage server -- `Put(key,value),` `Get(key)`->value
- Uses Go's RPC library
- Common:
  - Declare Args and Reply struct for each server handler.
- Client:
  - `connect()`'s `Dial()` creates a TCP connection to the server
  - `get()` and `put()` are **client "stubs"**
  - `Call()` asks the RPC library to perform the call
    - you specify server function name, arguments, place to put reply
    - library marshalls args, sends request, waits, unmarshalls reply
    - return value from `Call()` indicates whether it got a reply
    - usually you'll also have a `reply.Err` indicating service-level failure
- Server:
  - Go requires server **to declare an object with methods as RPC handlers**
  - Server then registers that object with the RPC library
  - Server accepts TCP connections, gives them to RPC library
  - The RPC library
    - reads each request
    - creates a new goroutine for this request
    - unmarshalls request
    - looks up the named object (in table create by Register())
    - calls the object's named method (dispatch)
    - marshalls reply
    - writes reply on TCP connection
  - The server's Get() and Put() handlers
    - Must lock, since RPC library creates a new goroutine for each request
    - read args; modify reply

### A few details

- What does a failure look like to the client RPC library?

  - Simplest failure-handling scheme: "best-effort RPC"

    - scheme
      - Call() waits for response for a while
      - If none arrives, re-send the request
      - Do this a few times
      - Then give up and return an error
    - is best effort ever OK?
      - read-only operations
      - operations that do nothing if repeated
        - e.g. DB checks if record has already been inserted

  - Better RPC behavior: "at-most-once RPC"

    - idea: client re-sends if no answer;

      - server RPC code detects duplicate requests,
      - returns previous reply instead of re-running handler

    - Q: how to detect a duplicate request?

      - client includes unique ID (XID) with each request
        - uses same XID for re-send

    - server:

      - ```go
        if seen[xid]:
        	r = old[xid]
        else
        	r = handler()
        	old[xid] = r
        	seen[xid] = true
        ```

    - some at-most-once complexities

      - *<u>see notes</u>*

    - Go RPC is a simple form of "at-most-once"


### *Thinking*

- 简要介绍了Go的一些并行计算的特征
- 通过Web Crawler对比了state lock模型和channel模型的区别
  - 如果是状态的改变使用lock
  - 如果是等待和通知使用channel
- 笔记中详细介绍了RPC，是分布式系统的一个关键机制，
  - 易于编程的客户/服务器通信、隐藏网络协议的细节、将数据（字符串、数组、map等）转换成 "wire format"
  - Go本身有实现RPC的package

# L3. The Google File System

### Why Hard

- Performance -> Sharding
- Many servers -> Fault
- -> Replication
- -> Inconsistencies
- Conssitency -> Low performance

## GFS

- Big, Fast (Paralell Access)
- Global
- Sharding (High Aggregate Speed)
- Automatic recovery
- Single data center
- Internal use
- Big sequential access

### Design

- Client
- Master
- Chunkserver

### Master Data

- file name -> 
  - array of chunk handles (non-volatile)
- chunk handles -> 
  - list of chunk servers
  - version # (non-volatile)
  - primary
  - lease expiration
- log, checkpoint -> 
  - disk
    - log is faster than b-tree in disk
    - after a master fail, new master only read log from when the checkpoint was created

### Read

1. client -> name, offset -> master
2. master -> handles, list of severs -> client (cached)
3. client -> request -> chunkserver
4. chunkserver -> data -> client

### Write (Record Append)

- on master
  - if no primary
    - find up-to-date replicas
    - pick primary
    - increment version #
    - version # -> primary and secondaries
    - lease -> primary
- primary picks offset
- all replicas told to write at offset
- if all "yes", "success" -> client
  else, "no" -> client



## Paper

### The Google File System

<img src="image.assets/Screen Shot 2022-06-21 at 15.31.26.png" alt="Screen Shot 2022-06-21 at 15.31.26" style="zoom: 50%;" />

1. **Introduction**
2. **Design Overview**
   1. Assumptions
   2. Interface
   3. Architecture
   4. Single Master
   5. Chunk Size
   6. Metadata
      1. In-Memory Data Structures
      2. Chunk Locations
      3. Operation Log
   7. Consistency Model
      1. Guarantees by GFS
      2. Implications for Applications
3. **System Interactions**
   1. Leases and Mutation Order
      - ​	<img src="image.assets/Screen Shot 2022-06-21 at 15.30.53.png" alt="Screen Shot 2022-06-21 at 15.30.53" style="zoom: 33%;" />
   2. Data Flow
   3. Atomic Record Appends
   4. Snapshot
4. **Master Operations**
   1. Namespace Management and Locking
   2. Replica Placement
   3. Creation, Re-replication, Rebalancing
   4. Garbage Collection
      1. Mechanism
      2. Discussion
   5. Stale Replica Detection
5. **Fault Tolerance and Diagnosis**
   1. High Availability
      1. Fast Revovery
      2. Chunk Replication
      3. Master Replication
   2. Data Integrity
   3. Diagnostic Tools
6. **Measurements**
   1. Micro-benchmarks
      1. Reads
      2. Writes
      3. Record Appends
   2. Real World Clusters
      1. Storage
      2. Metadata
      3. Read and Write Rates
      4. Master Load
      5. Recovery Time
   3. Workload Breakdown
      1. Methodology and Caveats
      2. Chunkserver Workload
      3. Appends versus Writes
      4. Master Workload
7. **Experiences**
8. **Related Work**
9. **Conclusions**

### *Summary*

- Performance
  - Sharding
  - Separating file system control from data transfer (which passes directly between chunkservers and clients)
  - --Single coordinator performance
- Consistency
  - Primary to sequence writes
  - Leases to prevent split-brain chunkserver primaries
  - --Too relaxed consistency
- --Single Data Center

### *Thinking*

- 这篇2003年的论文介绍了Google File System，一个分布式存储系统
  - 介绍了一些符合实际应用需求的设计决策
    - 如将文件系统管理和数据传输分离，前者通过primary到secondaries，而后者直接在各server间传递（避免树状传递浪费网络带宽）



# L4. Primary-Backup Replication

### Failures

- yes: fail-stop failures
- no: logic bugs, configuration errors, malicious
- maybe: earthquake...

### Challenges

- has primary failed?
  - split-brain system
- keep primary and backup in sync
  - apply changes in order
  - deal with non-determinism
- fail over

### Two Approaches

1. State transfer
   - disadvantage: if a operation generates many states, it's expensive to send many data
2. Replicate state machine (RSM)

### Level of operations to replicate

- application-level operations (file append, write)
- machine level
  - transparent!
  - virtual machines

## VMFT: exploit virtualization

- transparent replication
- appears to client that server is a single machine
- VMware product

### Overview

- killover

### Goal: behave like a single machine

- divergence source
  - non-deterministic instruction
  - input packets
  - timer interrupts
  - multicore -> disallow

### Interrupts

- ++*<u>question</u>*: deterministic instructions are not sent throught the logging channel

### Output Rule

- backup acknowledgment



## Paper

### The Design of a Practical System for Fault-Tolerant Virtual Machines

<img src="image.assets/Screen Shot 2022-08-03 at 10.30.37.png" alt="Screen Shot 2022-08-03 at 10.30.37" style="zoom:50%;" />

1. **INTRODUCTION**
2. **BASIC FT DESIGN**
   1. Deterministic Replay Implementation
   2. FT Protocal
   3. Detecting and Responding to Failure
3. **PRACTICAL IMPLEMENTATION OF FT**
   1. Starting and Restarting FT VMs
   2. Managing the Logging Channel
   3. Operations on FT VMS
   4. Implementation Issues for Disk IOs
   5. Implementation Issues for Network IO
4. **DESIGN ALTERNATIVES**
   1. Shared vs. Non-shared Disk
   2. Executing Disk Reads on the Backup VM
5. **PERFORMANCE EVALUATION**
   1. Basic Performance Results
   2. Network Benchmarks
6. **RELATED WORK**
7. **CONCLUSION AND FUTURE WORK**

### *Summary*

- Fault Tolerance
  - VMware deterministic reply --> Machine-level RSM (replicated state machine)
- Performance
  - --Strict output rule (vs. application-level RSM)

### *Thinking*

- 介绍了VMware开发的一个基于虚拟机的容错（fault-tolerant）系统
  - 核心是传输一系列的决定性的指令，而非数据
    - 非决定性指令转化为决定性的指令（基于虚拟机）
  - 一些设计决定的讨论
    - 硬盘读操作传输数据
    - 使用同一个共享文件系统

















